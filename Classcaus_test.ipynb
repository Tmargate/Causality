{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b31f47-51f0-4e4b-862e-5613a984a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "#import lightgbm as lgbm\n",
    "from sklearn import metrics\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "# Importing the utils.py file from the class_files app.\n",
    "from utils import *\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "import torchtuples as tt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da781821-be63-43df-8d02-de82b33fd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Type_simulation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d204a08-92b2-463a-af35-d88fd92793da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "\n",
    "    def __init__(self,  alpha, beta=1):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.loss_classif = nn.BCELoss()\n",
    "        self.loss_wass = WassLoss()  # IPM\n",
    "\n",
    "    def forward(self, phi_t, sigma, y_train):\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        sigma = sigma.reshape(-1, 1)\n",
    "        loss_classif = self.loss_classif(sigma, y_train)\n",
    "        loss_wass = self.loss_wass(phi_t)  # Wasserstein Loss\n",
    "        self.wd = loss_wass.item()\n",
    "        self.cl = loss_classif.item()\n",
    "        return self.beta*loss_classif + self.alpha * loss_wass  #\n",
    "\n",
    "\n",
    "# It creates a class that inherits from nn.Module.\n",
    "class WassLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, psi: Tensor) -> Tensor:\n",
    "        a, b = sepr_repr(psi)\n",
    "        self.psi0 = a\n",
    "        self.psi1 = b\n",
    "        return SinkhornDistance(eps=0.001, max_iter=100, reduction=None)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f9002b9-68fa-426e-8c5f-dc05f3603a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetClassif(nn.Module):\n",
    "    def __init__(self, in_features, encoded_features):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(in_features-1, 32),  nn.LeakyReLU(),\n",
    "            nn.Linear(32, 32),  nn.ReLU(),\n",
    "            nn.Linear(32, 28),  nn.LeakyReLU(),\n",
    "            nn.Linear(28, encoded_features),\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Linear(encoded_features + 1, 128), nn.LeakyReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 50),  nn.ReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "        )\n",
    "        self.loss_classif = nn.BCELoss()\n",
    "        self.loss_wass = WassLoss()  # IPM\n",
    "\n",
    "    def forward(self, input):\n",
    "        x, t = get_data(input)\n",
    "        self.input = input\n",
    "        t = t.reshape(-1, 1)\n",
    "        phi = self.phi(x)\n",
    "        phi_t = torch.cat((phi, t), 1)\n",
    "        sigma = nn.Sigmoid()(self.psi(phi_t))\n",
    "        return phi_t, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04a08d65-dca4-488c-b461-ab694a15ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifBase(tt.Model):\n",
    "    \"\"\"Base class for classification models.\n",
    "    Essentially same as torchtuples.Model,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, loss=None, optimizer=None, device=None):\n",
    "        super().__init__(net, loss, optimizer, device)\n",
    "\n",
    "    def predict_proba(self, input,  **kwargs):\n",
    "        x, t = get_data(input)\n",
    "        self.input = input\n",
    "        t = t.reshape(-1, 1)\n",
    "        phi = self.net.phi(x)\n",
    "        phi_t = torch.cat((phi, t), 1)\n",
    "        sigma = nn.Sigmoid()(self.net.psi(phi_t))\n",
    "        return sigma.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44774b1e-3a96-4889-87aa-f66605dbe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.path = \"dataclassif\"  # params_sim['path_data']\n",
    "\n",
    "    def load_data_sim_benchmark(self):\n",
    "\n",
    "        df = pd.read_csv(self.path + \".csv\")\n",
    "        df = reduce_mem_usage(df)\n",
    "        self.df = df\n",
    "        dim = df.shape[1]-9\n",
    "\n",
    "        x_z_list = [\"X\" + str(i) for i in range(1, dim + 1)] + [\"tt\"]\n",
    "        leave = x_z_list + [\"Y_f\", \"Y_cf\", \"Y_0\", \"Y_1\", \"pi_0\", \"pi_1\"]\n",
    "\n",
    "        ##\n",
    "        rs = ShuffleSplit(test_size=.4, random_state=0)\n",
    "        df_ = df[leave].copy()\n",
    "\n",
    "        for train_index, test_index in rs.split(df_):\n",
    "            df_train = df_.drop(test_index)\n",
    "            df_test = df_.drop(train_index)\n",
    "            df_val = df_test.sample(frac=0.2)\n",
    "            df_test = df_test.drop(df_val.index)\n",
    "\n",
    "        counter_list = [\"Y_cf\", \"Y_0\", \"Y_1\", \"pi_0\", \"pi_1\"]\n",
    "        y_train_classif = df_train['Y_f'].values.astype(\"float32\")\n",
    "        y_val_classif = df_val['Y_f'].values.astype(\"float32\")\n",
    "        y_test_classif = df_test['Y_f'].values.astype(\"float32\")\n",
    "\n",
    "        counter_train = df_train[counter_list].values.astype(\"float32\")\n",
    "        counter_val = df_val[counter_list].values.astype(\"float32\")\n",
    "        counter_test = df_test[counter_list].values.astype(\"float32\")\n",
    "\n",
    "        train = (df_train[x_z_list].values.astype(\"float32\"), y_train_classif)\n",
    "        val = (\n",
    "            df_val[x_z_list].values.astype(\"float32\"),\n",
    "            y_val_classif,\n",
    "        )\n",
    "\n",
    "        x_test = df_test[x_z_list].values.astype(\"float32\")\n",
    "\n",
    "        # SPlit data for OURS\n",
    "        self.x_train, self.y_train, self.train, self.val,\\\n",
    "            self.y_test, self.x_test, self.counter_train, self.counter_test, self.counter_val = \\\n",
    "            train[0], train[1], train, val, y_test_classif, x_test, counter_train, counter_test, counter_val\n",
    "\n",
    "        self.x_train = torch.from_numpy(self.x_train).float()\n",
    "        self.y_train = torch.from_numpy(self.y_train).float().view(-1, 1)\n",
    "        self.x_val = torch.from_numpy(self.val[0]).float()\n",
    "        self.y_val = torch.from_numpy(self.val[1]).float().view(-1, 1)\n",
    "        self.x_test = torch.from_numpy(self.x_test).float()\n",
    "        self.y_test = torch.from_numpy(self.y_test).float().view(-1, 1)\n",
    "\n",
    "        #  SPlit data for benchmarking\n",
    "\n",
    "        def get_separ_data(x):\n",
    "            mask_1 = x[\"tt\"] == 1\n",
    "            mask_0 = x[\"tt\"] == 0\n",
    "            x_1 = x[mask_1].drop(columns=\"tt\")\n",
    "            x_0 = x[mask_0].drop(columns=\"tt\")\n",
    "            return x_0, x_1\n",
    "\n",
    "        df_train_0,  df_train_1 = get_separ_data(df_train)\n",
    "        df_val_0, df_val_1 = get_separ_data(df_val)\n",
    "        x_z_list = [\"X\" + str(i) for i in range(1, dim + 1)]\n",
    "        self.x_train_0 = torch.from_numpy(\n",
    "            df_train_0[x_z_list].values.astype(\"float32\")).float()\n",
    "        self.x_train_1 = torch.from_numpy(\n",
    "            df_train_1[x_z_list].values.astype(\"float32\")).float()\n",
    "        self.x_val_0 = torch.from_numpy(\n",
    "            df_val_0[x_z_list].values.astype(\"float32\")).float()\n",
    "        self.x_val_1 = torch.from_numpy(\n",
    "            df_val_1[x_z_list].values.astype(\"float32\")).float()\n",
    "\n",
    "        self.y_train_0 = torch.from_numpy(\n",
    "            df_train_0[\"Y_f\"].values.astype(\"float32\")).float().view(-1, 1)\n",
    "        self.y_train_1 = torch.from_numpy(\n",
    "            df_train_1[\"Y_f\"].values.astype(\"float32\")).float().view(-1, 1)\n",
    "        self.y_val_0 = torch.from_numpy(\n",
    "            df_val_0[\"Y_f\"].values.astype(\"float32\")).float().view(-1, 1)\n",
    "        self.y_val_1 = torch.from_numpy(\n",
    "            df_val_1[\"Y_f\"].values.astype(\"float32\")).float().view(-1, 1)\n",
    "\n",
    "        self.counter_train_0 = torch.from_numpy(\n",
    "            df_train_0[counter_list].values.astype(\"float32\")).float()\n",
    "        self.counter_train_1 = torch.from_numpy(\n",
    "            df_train_1[counter_list].values.astype(\"float32\")).float()\n",
    "        self.counter_val_0 = torch.from_numpy(\n",
    "            df_val_0[counter_list].values.astype(\"float32\")).float()\n",
    "        self.counter_val_1 = torch.from_numpy(\n",
    "            df_val_1[counter_list].values.astype(\"float32\")).float()\n",
    "\n",
    "    def get_data(self):\n",
    "        self.load_data_sim_benchmark()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd25f069-e21b-4229-a1c4-a44c697c1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifCaus(nn.Module):\n",
    "\n",
    "    def __init__(self, params_classifcaus):\n",
    "        super().__init__()\n",
    "        encoded_features = params_classifcaus['encoded_features']\n",
    "        alpha_wass = params_classifcaus['alpha_wass']\n",
    "        batch_size = params_classifcaus['batch_size']\n",
    "        epochs = params_classifcaus['epochs']\n",
    "        lr = params_classifcaus['lr']\n",
    "        patience = params_classifcaus['patience']\n",
    "\n",
    "        self.data = DataLoader().get_data()\n",
    "\n",
    "        self.in_features = self.data.x_train.shape[1]\n",
    "        self.encoded_features = encoded_features\n",
    "        self.net = NetClassif(self.in_features, self.encoded_features)\n",
    "\n",
    "        self.alpha_wass = alpha_wass\n",
    "        self.lr = lr\n",
    "        self.loss = Loss(self.alpha_wass, 1)\n",
    "        self.metrics = dict(loss_classif=Loss(0, 1), loss_wass=Loss(1, 0))\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.model = ClassifBase(\n",
    "            net=self.net, loss=self.loss, optimizer=torch.optim.Adam, device=None)\n",
    "\n",
    "        self.patence = patience\n",
    "        self.callbacks = [tt.cb.EarlyStopping(patience=patience)]\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit_model(self):\n",
    "        self.model.fit(input=self.data.x_train, target=self.data.y_train,\n",
    "                       val_data=(self.data.x_val, self.data.y_val),\n",
    "                       batch_size=self.batch_size,\n",
    "                       epochs=self.epochs,\n",
    "                       callbacks=self.callbacks,\n",
    "                       metrics=self.metrics)\n",
    "        return self\n",
    "\n",
    "    def pred_t(self, x_test, t, y_test, p_test):\n",
    "        # drop x_test[:,-1] and replace it by t * 1.\n",
    "        tt = torch.ones(x_test.shape[0], 1) * t\n",
    "        x_test_t = np.concatenate((x_test[:, :-1], tt.reshape(-1, 1)), axis=1)\n",
    "        x_test_t = torch.from_numpy(x_test_t).float()\n",
    "        y_test = y_test.squeeze().numpy()\n",
    "        p_pred_t = self.model.predict_proba(x_test_t).squeeze()\n",
    "        y_pred_t = (p_pred_t > 0.5) * 1.0\n",
    "        acc = metrics.accuracy_score(y_test, y_pred_t)\n",
    "        cf_m = metrics.confusion_matrix(y_test, y_pred_t)\n",
    "        f1_s = metrics.f1_score(y_test, y_pred_t)\n",
    "        auc = metrics.roc_auc_score(y_test, p_pred_t)\n",
    "        kl = KL(p_test, p_pred_t)\n",
    "        std_diff =std_diff_metric(p_test, p_pred_t)\n",
    "        report = classification_report(y_test, y_pred_t)\n",
    "        return acc, cf_m, f1_s, auc, kl, p_pred_t, report, std_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d7afdaa4-b514-4489-af4c-48adddd1e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_classifcaus = {\n",
    "    \"encoded_features\": 25,\n",
    "    \"alpha_wass\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 0.001,\n",
    "    \"patience\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6b6af83-9d17-4860-814e-3a03f9618f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sim = {\n",
    "    'n_features': 25,\n",
    "    'n_classes': 2,\n",
    "    'n_samples': 1000,\n",
    "    'beta': [0.1, 0.1, 0.3],\n",
    "    'coef_tt': 1.1,\n",
    "    'rho': 15\n",
    "}\n",
    "idx = np.arange(param_sim['n_features'])\n",
    "param_sim['beta'] = (-1) ** idx * np.exp(-idx / 20.)\n",
    "param_sim['alpha'] = (-1) ** idx * np.exp(-idx / 20.) /3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f99c83b-0226-4124-8862-9bb59f1a26f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>tt</th>\n",
       "      <th>Y_f</th>\n",
       "      <th>Y_cf</th>\n",
       "      <th>Y_0</th>\n",
       "      <th>Y_1</th>\n",
       "      <th>pi_0</th>\n",
       "      <th>pi_1</th>\n",
       "      <th>pi_f</th>\n",
       "      <th>pi_cf</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840774</td>\n",
       "      <td>-2.953608</td>\n",
       "      <td>-1.894213</td>\n",
       "      <td>2.387962</td>\n",
       "      <td>4.276957</td>\n",
       "      <td>1.813672</td>\n",
       "      <td>0.734181</td>\n",
       "      <td>-0.534431</td>\n",
       "      <td>-1.120795</td>\n",
       "      <td>-0.662566</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073326</td>\n",
       "      <td>0.192060</td>\n",
       "      <td>0.073326</td>\n",
       "      <td>0.192060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.164448</td>\n",
       "      <td>2.710316</td>\n",
       "      <td>-0.298409</td>\n",
       "      <td>0.098235</td>\n",
       "      <td>5.597416</td>\n",
       "      <td>1.106360</td>\n",
       "      <td>2.991877</td>\n",
       "      <td>-2.866985</td>\n",
       "      <td>3.994747</td>\n",
       "      <td>-1.915142</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968901</td>\n",
       "      <td>0.989429</td>\n",
       "      <td>0.989429</td>\n",
       "      <td>0.968901</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.311295</td>\n",
       "      <td>-2.982420</td>\n",
       "      <td>-3.726019</td>\n",
       "      <td>-2.025751</td>\n",
       "      <td>2.266880</td>\n",
       "      <td>2.895718</td>\n",
       "      <td>-0.334845</td>\n",
       "      <td>-6.615137</td>\n",
       "      <td>-3.001232</td>\n",
       "      <td>5.555681</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.817807</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.930962</td>\n",
       "      <td>0.817807</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.303825</td>\n",
       "      <td>-6.269872</td>\n",
       "      <td>-2.014411</td>\n",
       "      <td>-5.835530</td>\n",
       "      <td>-3.326133</td>\n",
       "      <td>-8.582165</td>\n",
       "      <td>-3.893511</td>\n",
       "      <td>-3.805137</td>\n",
       "      <td>-5.461583</td>\n",
       "      <td>-1.689110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.890676</td>\n",
       "      <td>3.721534</td>\n",
       "      <td>0.714977</td>\n",
       "      <td>0.218185</td>\n",
       "      <td>-2.493685</td>\n",
       "      <td>-0.372298</td>\n",
       "      <td>-0.862002</td>\n",
       "      <td>1.951581</td>\n",
       "      <td>0.884283</td>\n",
       "      <td>1.662861</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.184167</td>\n",
       "      <td>0.404110</td>\n",
       "      <td>0.404110</td>\n",
       "      <td>0.184167</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>6.626844</td>\n",
       "      <td>-2.259308</td>\n",
       "      <td>3.129076</td>\n",
       "      <td>3.421506</td>\n",
       "      <td>4.722830</td>\n",
       "      <td>2.061740</td>\n",
       "      <td>3.738173</td>\n",
       "      <td>4.188156</td>\n",
       "      <td>9.450981</td>\n",
       "      <td>3.691438</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-4.317431</td>\n",
       "      <td>-1.281455</td>\n",
       "      <td>4.359847</td>\n",
       "      <td>1.529762</td>\n",
       "      <td>-3.497465</td>\n",
       "      <td>3.134878</td>\n",
       "      <td>2.234285</td>\n",
       "      <td>3.315011</td>\n",
       "      <td>6.942328</td>\n",
       "      <td>3.291208</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980267</td>\n",
       "      <td>0.993344</td>\n",
       "      <td>0.993344</td>\n",
       "      <td>0.980267</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7.147107</td>\n",
       "      <td>-0.822083</td>\n",
       "      <td>-4.650697</td>\n",
       "      <td>-0.139623</td>\n",
       "      <td>1.085023</td>\n",
       "      <td>1.027633</td>\n",
       "      <td>0.393668</td>\n",
       "      <td>-0.454934</td>\n",
       "      <td>-1.117605</td>\n",
       "      <td>0.341220</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998410</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.998410</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.151042</td>\n",
       "      <td>-1.942032</td>\n",
       "      <td>4.027250</td>\n",
       "      <td>2.555851</td>\n",
       "      <td>-2.475364</td>\n",
       "      <td>-0.590037</td>\n",
       "      <td>-2.048190</td>\n",
       "      <td>0.444736</td>\n",
       "      <td>-0.194229</td>\n",
       "      <td>-2.258305</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962206</td>\n",
       "      <td>0.987094</td>\n",
       "      <td>0.962206</td>\n",
       "      <td>0.987094</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.190889</td>\n",
       "      <td>0.557804</td>\n",
       "      <td>0.809514</td>\n",
       "      <td>-2.466342</td>\n",
       "      <td>1.291220</td>\n",
       "      <td>0.982822</td>\n",
       "      <td>-1.173245</td>\n",
       "      <td>-2.155636</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>1.277880</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.290822</td>\n",
       "      <td>0.551963</td>\n",
       "      <td>0.290822</td>\n",
       "      <td>0.551963</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.840774 -2.953608 -1.894213  2.387962  4.276957  1.813672  0.734181   \n",
       "1    1.164448  2.710316 -0.298409  0.098235  5.597416  1.106360  2.991877   \n",
       "2    1.311295 -2.982420 -3.726019 -2.025751  2.266880  2.895718 -0.334845   \n",
       "3    0.303825 -6.269872 -2.014411 -5.835530 -3.326133 -8.582165 -3.893511   \n",
       "4    3.890676  3.721534  0.714977  0.218185 -2.493685 -0.372298 -0.862002   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  6.626844 -2.259308  3.129076  3.421506  4.722830  2.061740  3.738173   \n",
       "996 -4.317431 -1.281455  4.359847  1.529762 -3.497465  3.134878  2.234285   \n",
       "997  7.147107 -0.822083 -4.650697 -0.139623  1.085023  1.027633  0.393668   \n",
       "998  1.151042 -1.942032  4.027250  2.555851 -2.475364 -0.590037 -2.048190   \n",
       "999 -0.190889  0.557804  0.809514 -2.466342  1.291220  0.982822 -1.173245   \n",
       "\n",
       "           X8        X9       X10  ...  tt  Y_f  Y_cf  Y_0  Y_1      pi_0  \\\n",
       "0   -0.534431 -1.120795 -0.662566  ...   0    0     0    0    0  0.073326   \n",
       "1   -2.866985  3.994747 -1.915142  ...   1    1     1    1    1  0.968901   \n",
       "2   -6.615137 -3.001232  5.555681  ...   1    1     1    1    1  0.817807   \n",
       "3   -3.805137 -5.461583 -1.689110  ...   0    1     1    1    1  0.999525   \n",
       "4    1.951581  0.884283  1.662861  ...   1    0     0    0    0  0.184167   \n",
       "..        ...       ...       ...  ...  ..  ...   ...  ...  ...       ...   \n",
       "995  4.188156  9.450981  3.691438  ...   1    1     1    1    1  0.999997   \n",
       "996  3.315011  6.942328  3.291208  ...   1    1     1    1    1  0.980267   \n",
       "997 -0.454934 -1.117605  0.341220  ...   1    1     1    1    1  0.998410   \n",
       "998  0.444736 -0.194229 -2.258305  ...   0    1     1    1    1  0.962206   \n",
       "999 -2.155636  0.021827  1.277880  ...   0    0     1    0    1  0.290822   \n",
       "\n",
       "         pi_1      pi_f     pi_cf   id  \n",
       "0    0.192060  0.073326  0.192060    0  \n",
       "1    0.989429  0.989429  0.968901    1  \n",
       "2    0.930962  0.930962  0.817807    2  \n",
       "3    0.999842  0.999525  0.999842    3  \n",
       "4    0.404110  0.404110  0.184167    4  \n",
       "..        ...       ...       ...  ...  \n",
       "995  0.999999  0.999999  0.999997  995  \n",
       "996  0.993344  0.993344  0.980267  996  \n",
       "997  0.999470  0.999470  0.998410  997  \n",
       "998  0.987094  0.962206  0.987094  998  \n",
       "999  0.551963  0.290822  0.551963  999  \n",
       "\n",
       "[1000 rows x 35 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simu = Simulation_mod(param_sim)\n",
    "simu.simule(wd_para = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f170f973-9fea-4f32-b8bf-b7edbc170d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n"
     ]
    }
   ],
   "source": [
    "classcaus = ClassifCaus (params_classifcaus= params_classifcaus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a20dfcd-ed45-4d87-bed5-b33db622f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader().get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2209bd7b-53a1-464a-819d-4d8840eff0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 26])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efbfb3cf-3b24-4a32-b3e3-43ee7bd3d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 25])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classcaus.data.x_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "38a09c4a-1f52-45f7-8816-966323f4bfa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mType_simulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBenchmark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Benchmark'"
     ]
    }
   ],
   "source": [
    "from Type_simulation import *\n",
    "from Benchmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7065d16d-dca1-4e72-b451-557e54d5fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_classifcaus = {\n",
    "    \"encoded_features\": 25,\n",
    "    \"alpha_wass\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 0.001,\n",
    "    \"patience\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f19e6056-7890-4a88-8156-d9ce05770054",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sim = {\n",
    "    'n_features': 25,\n",
    "    'n_classes': 2,\n",
    "    'n_samples': 1000,\n",
    "    'beta': [0.1, 0.1, 0.3],\n",
    "    'coef_tt': 1.1,\n",
    "    'rho': 15\n",
    "}\n",
    "idx = np.arange(param_sim['n_features'])\n",
    "param_sim['beta'] = (-1) ** idx * np.exp(-idx / 20.)\n",
    "param_sim['alpha'] = (-1) ** idx * np.exp(-idx / 20.) /3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0aa2a618-0245-4c02-bc8e-d31de63c1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_param_classcaus = {\n",
    "    \"batch_size\" : params_classifcaus[\"batch_size\"],\n",
    "    \"epochs\" : params_classifcaus[\"epochs\"],\n",
    "    \"callbacks\" : [tt.cb.EarlyStopping(patience=params_classifcaus[\"patience\"])],\n",
    "    \"metrics\" : dict(loss_classif=Loss(0, 1), loss_wass=Loss(1, 0))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "27e784ba-35f2-4f04-95b2-cae95f4020d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simu = Simulation_mod(param_sim)\n",
    "data = simu.simule(wd_para=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "621b368a-2a95-4794-947c-30c037635539",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shift_tt = np.concatenate((simu.X_shift, simu.tt.reshape(simu.n_samples,1)), axis = 1)\n",
    "outcome_f = np.asarray(data[\"Y_f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29f39082-2a82-4a80-a7d1-ee07af27665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.26 MB\n",
      "Memory usage after optimization is: 0.06 MB\n",
      "Decreased by 76.8%\n"
     ]
    }
   ],
   "source": [
    "classcaus = ClassifCaus(params_classifcaus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3e6f1a44-69f9-4a54-9640-678937329662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 0.6920,\ttrain_loss_classif: 0.6912,\ttrain_loss_wass: 0.0837,\tval_loss: 0.6890,\tval_loss_classif: 0.6883,\tval_loss_wass: 0.0661\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.6878,\ttrain_loss_classif: 0.6874,\ttrain_loss_wass: 0.0473,\tval_loss: 0.6851,\tval_loss_classif: 0.6844,\tval_loss_wass: 0.0727\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 0.6828,\ttrain_loss_classif: 0.6822,\ttrain_loss_wass: 0.0610,\tval_loss: 0.6793,\tval_loss_classif: 0.6782,\tval_loss_wass: 0.1106\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 0.6758,\ttrain_loss_classif: 0.6745,\ttrain_loss_wass: 0.1249,\tval_loss: 0.6715,\tval_loss_classif: 0.6700,\tval_loss_wass: 0.1466\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.6681,\ttrain_loss_classif: 0.6660,\ttrain_loss_wass: 0.2138,\tval_loss: 0.6616,\tval_loss_classif: 0.6589,\tval_loss_wass: 0.2675\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.6557,\ttrain_loss_classif: 0.6534,\ttrain_loss_wass: 0.2264,\tval_loss: 0.6550,\tval_loss_classif: 0.6523,\tval_loss_wass: 0.2695\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.6501,\ttrain_loss_classif: 0.6483,\ttrain_loss_wass: 0.1799,\tval_loss: 0.6509,\tval_loss_classif: 0.6492,\tval_loss_wass: 0.1720\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 0.6433,\ttrain_loss_classif: 0.6424,\ttrain_loss_wass: 0.0912,\tval_loss: 0.6450,\tval_loss_classif: 0.6436,\tval_loss_wass: 0.1366\n",
      "8:\t[0s / 0s],\t\ttrain_loss: 0.6326,\ttrain_loss_classif: 0.6316,\ttrain_loss_wass: 0.1049,\tval_loss: 0.6380,\tval_loss_classif: 0.6366,\tval_loss_wass: 0.1447\n",
      "9:\t[0s / 0s],\t\ttrain_loss: 0.6233,\ttrain_loss_classif: 0.6223,\ttrain_loss_wass: 0.0989,\tval_loss: 0.6269,\tval_loss_classif: 0.6255,\tval_loss_wass: 0.1413\n",
      "10:\t[0s / 0s],\t\ttrain_loss: 0.6028,\ttrain_loss_classif: 0.6017,\ttrain_loss_wass: 0.1093,\tval_loss: 0.6157,\tval_loss_classif: 0.6143,\tval_loss_wass: 0.1374\n",
      "11:\t[0s / 0s],\t\ttrain_loss: 0.5753,\ttrain_loss_classif: 0.5742,\ttrain_loss_wass: 0.1093,\tval_loss: 0.6005,\tval_loss_classif: 0.5990,\tval_loss_wass: 0.1421\n",
      "12:\t[0s / 0s],\t\ttrain_loss: 0.5425,\ttrain_loss_classif: 0.5413,\ttrain_loss_wass: 0.1199,\tval_loss: 0.6144,\tval_loss_classif: 0.6129,\tval_loss_wass: 0.1428\n",
      "13:\t[0s / 0s],\t\ttrain_loss: 0.5140,\ttrain_loss_classif: 0.5128,\ttrain_loss_wass: 0.1223,\tval_loss: 0.5793,\tval_loss_classif: 0.5778,\tval_loss_wass: 0.1459\n",
      "14:\t[0s / 0s],\t\ttrain_loss: 0.4741,\ttrain_loss_classif: 0.4727,\ttrain_loss_wass: 0.1346,\tval_loss: 0.5860,\tval_loss_classif: 0.5845,\tval_loss_wass: 0.1476\n",
      "15:\t[0s / 0s],\t\ttrain_loss: 0.4539,\ttrain_loss_classif: 0.4526,\ttrain_loss_wass: 0.1323,\tval_loss: 0.5834,\tval_loss_classif: 0.5818,\tval_loss_wass: 0.1526\n",
      "16:\t[0s / 0s],\t\ttrain_loss: 0.4360,\ttrain_loss_classif: 0.4349,\ttrain_loss_wass: 0.1094,\tval_loss: 0.5736,\tval_loss_classif: 0.5720,\tval_loss_wass: 0.1575\n",
      "17:\t[0s / 0s],\t\ttrain_loss: 0.4350,\ttrain_loss_classif: 0.4336,\ttrain_loss_wass: 0.1380,\tval_loss: 0.5381,\tval_loss_classif: 0.5366,\tval_loss_wass: 0.1479\n",
      "18:\t[0s / 1s],\t\ttrain_loss: 0.3962,\ttrain_loss_classif: 0.3949,\ttrain_loss_wass: 0.1273,\tval_loss: 0.5686,\tval_loss_classif: 0.5671,\tval_loss_wass: 0.1420\n",
      "19:\t[0s / 1s],\t\ttrain_loss: 0.3791,\ttrain_loss_classif: 0.3779,\ttrain_loss_wass: 0.1198,\tval_loss: 0.5296,\tval_loss_classif: 0.5282,\tval_loss_wass: 0.1435\n",
      "20:\t[0s / 1s],\t\ttrain_loss: 0.3538,\ttrain_loss_classif: 0.3526,\ttrain_loss_wass: 0.1151,\tval_loss: 0.5356,\tval_loss_classif: 0.5342,\tval_loss_wass: 0.1408\n",
      "21:\t[0s / 1s],\t\ttrain_loss: 0.3447,\ttrain_loss_classif: 0.3435,\ttrain_loss_wass: 0.1183,\tval_loss: 0.5519,\tval_loss_classif: 0.5505,\tval_loss_wass: 0.1412\n",
      "22:\t[0s / 1s],\t\ttrain_loss: 0.3357,\ttrain_loss_classif: 0.3346,\ttrain_loss_wass: 0.1125,\tval_loss: 0.5409,\tval_loss_classif: 0.5394,\tval_loss_wass: 0.1461\n",
      "23:\t[0s / 1s],\t\ttrain_loss: 0.3738,\ttrain_loss_classif: 0.3727,\ttrain_loss_wass: 0.1129,\tval_loss: 0.6064,\tval_loss_classif: 0.6050,\tval_loss_wass: 0.1443\n",
      "24:\t[0s / 1s],\t\ttrain_loss: 0.3461,\ttrain_loss_classif: 0.3448,\ttrain_loss_wass: 0.1317,\tval_loss: 0.6072,\tval_loss_classif: 0.6058,\tval_loss_wass: 0.1390\n",
      "25:\t[0s / 1s],\t\ttrain_loss: 0.3312,\ttrain_loss_classif: 0.3301,\ttrain_loss_wass: 0.1091,\tval_loss: 0.5379,\tval_loss_classif: 0.5366,\tval_loss_wass: 0.1387\n",
      "26:\t[0s / 1s],\t\ttrain_loss: 0.3135,\ttrain_loss_classif: 0.3123,\ttrain_loss_wass: 0.1112,\tval_loss: 0.5979,\tval_loss_classif: 0.5966,\tval_loss_wass: 0.1328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifCaus(\n",
       "  (net): NetClassif(\n",
       "    (phi): Sequential(\n",
       "      (0): Linear(in_features=25, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=28, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "      (6): Linear(in_features=28, out_features=25, bias=True)\n",
       "    )\n",
       "    (psi): Sequential(\n",
       "      (0): Linear(in_features=26, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=128, out_features=50, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=50, out_features=1, bias=True)\n",
       "    )\n",
       "    (loss_classif): BCELoss()\n",
       "    (loss_wass): WassLoss()\n",
       "  )\n",
       "  (loss): Loss(\n",
       "    (loss_classif): BCELoss()\n",
       "    (loss_wass): WassLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classcaus.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cb75a7-475d-407c-9c58-ada593105ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Type_simulation import *\n",
    "from Benchmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96035d6f-f07f-470b-a1c9-97290c45773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sim = {\n",
    "    'n_features': 25,\n",
    "    'n_classes': 2,\n",
    "    'n_samples': 1000,\n",
    "    'beta': [0.1, 0.1, 0.3],\n",
    "    'coef_tt': 1,\n",
    "    'rho': 15\n",
    "}\n",
    "idx = np.arange(param_sim['n_features'])\n",
    "param_sim['beta'] = (-1) ** idx * np.exp(-idx / 20.)\n",
    "param_sim['alpha'] = (-1) ** idx * np.exp(-idx / 20.) /3\n",
    "\n",
    "params_classifcaus = {\n",
    "    \"encoded_features\": 25,\n",
    "    \"alpha_wass\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 0.001,\n",
    "    \"patience\": 7,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a761948-a2c1-4946-8291-2b79715e8b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>tt</th>\n",
       "      <th>Y_f</th>\n",
       "      <th>Y_cf</th>\n",
       "      <th>Y_0</th>\n",
       "      <th>Y_1</th>\n",
       "      <th>pi_0</th>\n",
       "      <th>pi_1</th>\n",
       "      <th>pi_f</th>\n",
       "      <th>pi_cf</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.582232</td>\n",
       "      <td>2.946846</td>\n",
       "      <td>0.328854</td>\n",
       "      <td>3.043115</td>\n",
       "      <td>3.018191</td>\n",
       "      <td>5.048214</td>\n",
       "      <td>-3.831565</td>\n",
       "      <td>-3.887028</td>\n",
       "      <td>1.430880</td>\n",
       "      <td>0.681834</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.932338</td>\n",
       "      <td>0.932338</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.738480</td>\n",
       "      <td>-2.836754</td>\n",
       "      <td>-2.582598</td>\n",
       "      <td>2.446942</td>\n",
       "      <td>1.907152</td>\n",
       "      <td>1.605542</td>\n",
       "      <td>-3.202857</td>\n",
       "      <td>-0.430623</td>\n",
       "      <td>8.329603</td>\n",
       "      <td>5.882438</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.737756</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0.737756</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.580838</td>\n",
       "      <td>-0.117542</td>\n",
       "      <td>1.340701</td>\n",
       "      <td>0.859052</td>\n",
       "      <td>1.722677</td>\n",
       "      <td>-3.081864</td>\n",
       "      <td>-7.941982</td>\n",
       "      <td>-5.880817</td>\n",
       "      <td>-0.658373</td>\n",
       "      <td>-2.750866</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809718</td>\n",
       "      <td>0.999750</td>\n",
       "      <td>0.809718</td>\n",
       "      <td>0.999750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.190883</td>\n",
       "      <td>1.875602</td>\n",
       "      <td>4.973408</td>\n",
       "      <td>8.703703</td>\n",
       "      <td>8.092241</td>\n",
       "      <td>0.449731</td>\n",
       "      <td>1.449753</td>\n",
       "      <td>8.063242</td>\n",
       "      <td>3.578876</td>\n",
       "      <td>-0.417627</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994192</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.994192</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.586204</td>\n",
       "      <td>-2.075733</td>\n",
       "      <td>10.249707</td>\n",
       "      <td>11.834051</td>\n",
       "      <td>2.600053</td>\n",
       "      <td>-1.764994</td>\n",
       "      <td>-7.184622</td>\n",
       "      <td>-5.379288</td>\n",
       "      <td>1.374429</td>\n",
       "      <td>9.632008</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.815832</td>\n",
       "      <td>-0.712875</td>\n",
       "      <td>-2.256611</td>\n",
       "      <td>-6.475482</td>\n",
       "      <td>-0.497559</td>\n",
       "      <td>-2.351044</td>\n",
       "      <td>2.772049</td>\n",
       "      <td>3.848931</td>\n",
       "      <td>0.864651</td>\n",
       "      <td>6.771720</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.988691</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.942261</td>\n",
       "      <td>-4.051602</td>\n",
       "      <td>-3.765366</td>\n",
       "      <td>5.065416</td>\n",
       "      <td>6.929367</td>\n",
       "      <td>3.157491</td>\n",
       "      <td>8.792586</td>\n",
       "      <td>0.854118</td>\n",
       "      <td>1.117248</td>\n",
       "      <td>-1.238569</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.461450</td>\n",
       "      <td>-0.121243</td>\n",
       "      <td>3.242743</td>\n",
       "      <td>0.685303</td>\n",
       "      <td>10.894464</td>\n",
       "      <td>-2.411443</td>\n",
       "      <td>-4.502560</td>\n",
       "      <td>-7.154692</td>\n",
       "      <td>-2.923953</td>\n",
       "      <td>-0.976134</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2.157961</td>\n",
       "      <td>9.763945</td>\n",
       "      <td>6.510948</td>\n",
       "      <td>0.769337</td>\n",
       "      <td>-1.108967</td>\n",
       "      <td>-2.415462</td>\n",
       "      <td>-2.650337</td>\n",
       "      <td>7.813920</td>\n",
       "      <td>2.307095</td>\n",
       "      <td>1.967788</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.919382</td>\n",
       "      <td>0.919382</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.806994</td>\n",
       "      <td>-6.034489</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>5.976691</td>\n",
       "      <td>8.306821</td>\n",
       "      <td>1.361662</td>\n",
       "      <td>1.147506</td>\n",
       "      <td>2.516772</td>\n",
       "      <td>5.040452</td>\n",
       "      <td>1.588928</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2         X3         X4         X5        X6        X7  \\\n",
       "0    4.582232  2.946846   0.328854   3.043115   3.018191  5.048214 -3.831565   \n",
       "1    1.738480 -2.836754  -2.582598   2.446942   1.907152  1.605542 -3.202857   \n",
       "2   -0.580838 -0.117542   1.340701   0.859052   1.722677 -3.081864 -7.941982   \n",
       "3   -0.190883  1.875602   4.973408   8.703703   8.092241  0.449731  1.449753   \n",
       "4    1.586204 -2.075733  10.249707  11.834051   2.600053 -1.764994 -7.184622   \n",
       "..        ...       ...        ...        ...        ...       ...       ...   \n",
       "995 -0.815832 -0.712875  -2.256611  -6.475482  -0.497559 -2.351044  2.772049   \n",
       "996 -1.942261 -4.051602  -3.765366   5.065416   6.929367  3.157491  8.792586   \n",
       "997  1.461450 -0.121243   3.242743   0.685303  10.894464 -2.411443 -4.502560   \n",
       "998  2.157961  9.763945   6.510948   0.769337  -1.108967 -2.415462 -2.650337   \n",
       "999  0.806994 -6.034489   0.963542   5.976691   8.306821  1.361662  1.147506   \n",
       "\n",
       "           X8        X9       X10  ...  tt  Y_f  Y_cf  Y_0  Y_1      pi_0  \\\n",
       "0   -3.887028  1.430880  0.681834  ...   1    1     0    0    1  0.014447   \n",
       "1   -0.430623  8.329603  5.882438  ...   1    1     1    1    1  0.737756   \n",
       "2   -5.880817 -0.658373 -2.750866  ...   0    1     1    1    1  0.809718   \n",
       "3    8.063242  3.578876 -0.417627  ...   1    1     1    1    1  0.994192   \n",
       "4   -5.379288  1.374429  9.632008  ...   1    0     0    0    0  0.000008   \n",
       "..        ...       ...       ...  ...  ..  ...   ...  ...  ...       ...   \n",
       "995  3.848931  0.864651  6.771720  ...   1    1     1    1    1  0.988691   \n",
       "996  0.854118  1.117248 -1.238569  ...   1    1     1    1    1  0.999975   \n",
       "997 -7.154692 -2.923953 -0.976134  ...   0    1     1    1    1  1.000000   \n",
       "998  7.813920  2.307095  1.967788  ...   1    1     0    0    1  0.011987   \n",
       "999  2.516772  5.040452  1.588928  ...   1    1     1    1    1  0.999875   \n",
       "\n",
       "         pi_1      pi_f     pi_cf   id  \n",
       "0    0.932338  0.932338  0.014447    0  \n",
       "1    0.999622  0.999622  0.737756    1  \n",
       "2    0.999750  0.809718  0.999750    2  \n",
       "3    0.999994  0.999994  0.994192    3  \n",
       "4    0.007826  0.007826  0.000008    4  \n",
       "..        ...       ...       ...  ...  \n",
       "995  0.999988  0.999988  0.988691  995  \n",
       "996  1.000000  1.000000  0.999975  996  \n",
       "997  1.000000  1.000000  1.000000  997  \n",
       "998  0.919382  0.919382  0.011987  998  \n",
       "999  1.000000  1.000000  0.999875  999  \n",
       "\n",
       "[1000 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simu = Simulation_mod(param_sim)\n",
    "simu.simule(wd_para = 0, function_type=\"linear\", coef_tt_const=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b19dd2-276d-4fbd-9d87-e69f261c5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "classcaus = Classcaus(simu, params_classifcaus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "592f6f5e-6b9c-4c3d-b1a9-40f02fd6abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_one = Benchmark(simu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56fb11a1-46a9-4253-8171-3982bf394dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_one.prep_bench(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31f44be1-2918-4fd1-875d-f18e04853aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([750, 26])\n",
      "torch.Size([750, 1])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.7000,\ttrain_loss_classif: 0.6990,\ttrain_loss_wass: 0.0990\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.6934,\ttrain_loss_classif: 0.6925,\ttrain_loss_wass: 0.0817\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 0.6881,\ttrain_loss_classif: 0.6875,\ttrain_loss_wass: 0.0667\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 0.6819,\ttrain_loss_classif: 0.6813,\ttrain_loss_wass: 0.0616\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.6712,\ttrain_loss_classif: 0.6705,\ttrain_loss_wass: 0.0670\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.6472,\ttrain_loss_classif: 0.6464,\ttrain_loss_wass: 0.0803\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.6038,\ttrain_loss_classif: 0.6028,\ttrain_loss_wass: 0.0984\n"
     ]
    }
   ],
   "source": [
    "bench_one.predict_one_model(classcaus, type_model=\"classcaus\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4998457c-e126-4f89-a040-597fd9ae9955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4600849 , 0.46022022, 0.45991755, 0.4599883 , 0.46011376,\n",
       "       0.45996228, 0.4603876 , 0.4600655 , 0.46084446, 0.46191   ,\n",
       "       0.46093684, 0.46071175, 0.46072772, 0.46052256, 0.46079496,\n",
       "       0.4598729 , 0.46067747, 0.46025932, 0.45996922, 0.46059334,\n",
       "       0.46121794, 0.46150365, 0.46144304, 0.46089572, 0.4604994 ,\n",
       "       0.46177396, 0.46282536, 0.46083337, 0.46164304, 0.46053782,\n",
       "       0.46027482, 0.45980552, 0.46095437, 0.4619468 , 0.46111867,\n",
       "       0.46031117, 0.46085444, 0.4607757 , 0.46046007, 0.46090555,\n",
       "       0.46080658, 0.46008176, 0.45998436, 0.46091932, 0.4611272 ,\n",
       "       0.46116617, 0.46220568, 0.4614463 , 0.46115994, 0.46159935,\n",
       "       0.46168736, 0.46232414, 0.4608562 , 0.4617321 , 0.46194124,\n",
       "       0.46200147, 0.46153057, 0.46095356, 0.46094388, 0.46041074,\n",
       "       0.46054342, 0.46022317, 0.46075055, 0.46050754, 0.46048346,\n",
       "       0.46020573, 0.45987582, 0.46235752, 0.46104944, 0.46012744,\n",
       "       0.4609094 , 0.4601944 , 0.46096137, 0.4614932 , 0.46010315,\n",
       "       0.46046263, 0.4604539 , 0.46020433, 0.4611433 , 0.46156296,\n",
       "       0.46019098, 0.4602938 , 0.46097457, 0.46052068, 0.46048686,\n",
       "       0.4601396 , 0.46039537, 0.46082082, 0.46140528, 0.46036732,\n",
       "       0.4596839 , 0.4605353 , 0.4617578 , 0.46080336, 0.46122083,\n",
       "       0.46070474, 0.46106312, 0.46171677, 0.46049622, 0.46102834,\n",
       "       0.46158412, 0.46171907, 0.4609879 , 0.46051902, 0.4623314 ,\n",
       "       0.4593032 , 0.46104303, 0.4608596 , 0.4610465 , 0.46175435,\n",
       "       0.45988974, 0.4611922 , 0.46034116, 0.46052948, 0.46023905,\n",
       "       0.4610103 , 0.46132877, 0.46063423, 0.45979396, 0.46114653,\n",
       "       0.46076518, 0.46147856, 0.460594  , 0.46012697, 0.46022645,\n",
       "       0.46033898, 0.46066797, 0.46051526, 0.46137115, 0.46087465,\n",
       "       0.46060267, 0.46032023, 0.45992252, 0.46019068, 0.46115315,\n",
       "       0.46134508, 0.46165416, 0.46117485, 0.4617767 , 0.46017206,\n",
       "       0.4600207 , 0.46054697, 0.46015584, 0.46083647, 0.46111208,\n",
       "       0.46028158, 0.46148798, 0.46017584, 0.45974982, 0.46011043,\n",
       "       0.46018863, 0.46175134, 0.46099663, 0.45966986, 0.4598611 ,\n",
       "       0.46011606, 0.4605453 , 0.4607844 , 0.46013188, 0.46042922,\n",
       "       0.46028915, 0.46058312, 0.45981392, 0.4595625 , 0.4598732 ,\n",
       "       0.46155956, 0.4606442 , 0.46039972, 0.46043044, 0.4610912 ,\n",
       "       0.46006602, 0.4602812 , 0.46031582, 0.4604549 , 0.4608382 ,\n",
       "       0.46094444, 0.46157637, 0.46096763, 0.4603408 , 0.46226332,\n",
       "       0.46175718, 0.4623403 , 0.4609558 , 0.46061823, 0.46202433,\n",
       "       0.46020928, 0.46022582, 0.46061218, 0.46087617, 0.46104568,\n",
       "       0.4607898 , 0.46046218, 0.4605622 , 0.46236312, 0.46039268,\n",
       "       0.4609871 , 0.46066588, 0.46159396, 0.4615891 , 0.46099302,\n",
       "       0.46110123, 0.46002018, 0.4605193 , 0.46108785, 0.46106908,\n",
       "       0.4608467 , 0.4603709 , 0.4611641 , 0.4608871 , 0.4601341 ,\n",
       "       0.4615765 , 0.4611717 , 0.46063945, 0.46006873, 0.46193925,\n",
       "       0.46034434, 0.46073887, 0.4609338 , 0.4601393 , 0.45994774,\n",
       "       0.46122813, 0.4609136 , 0.46043134, 0.46139556, 0.4610403 ,\n",
       "       0.46020916, 0.46067137, 0.46120983, 0.46030462, 0.46080446,\n",
       "       0.46139655, 0.46055916, 0.4603514 , 0.46118164, 0.4618779 ,\n",
       "       0.46085015, 0.4597211 , 0.4608748 , 0.46068096, 0.46159568,\n",
       "       0.4605344 , 0.46048525, 0.46144038, 0.46063635, 0.46101573,\n",
       "       0.46161318, 0.460647  , 0.45985773, 0.46070936, 0.45967415],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_one.predict_proba_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada4178c-0071-497b-9edd-69c7a628691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([750, 26])\n",
      "torch.Size([750, 1])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "0:\t[0s / 0s],\t\ttrain_loss: 0.7040,\ttrain_loss_classif: 0.7023,\ttrain_loss_wass: 0.1715\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.6806,\ttrain_loss_classif: 0.6791,\ttrain_loss_wass: 0.1579\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 0.6472,\ttrain_loss_classif: 0.6456,\ttrain_loss_wass: 0.1645\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 0.6238,\ttrain_loss_classif: 0.6219,\ttrain_loss_wass: 0.1909\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.6016,\ttrain_loss_classif: 0.5995,\ttrain_loss_wass: 0.2099\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.5791,\ttrain_loss_classif: 0.5770,\ttrain_loss_wass: 0.2142\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.5512,\ttrain_loss_classif: 0.5488,\ttrain_loss_wass: 0.2477\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accu_0</th>\n",
       "      <th>accu_1</th>\n",
       "      <th>MSE_p0</th>\n",
       "      <th>MSE_p1</th>\n",
       "      <th>PEHE</th>\n",
       "      <th>accu_sign_cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>classcaus</th>\n",
       "      <td>0.508</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.186359</td>\n",
       "      <td>0.235927</td>\n",
       "      <td>0.507797</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accu_0  accu_1    MSE_p0    MSE_p1      PEHE  accu_sign_cate\n",
       "classcaus   0.508   0.172  0.186359  0.235927  0.507797           0.016"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_one.benchmark_one_model(model=classcaus,type_model=\"classcaus\", name_model=\"classcaus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217b95ba-5359-4f9f-9463-f52b9ebce3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9315481e-03, -5.5617094e-04, -7.5587630e-04, -1.4562309e-03,\n",
       "       -2.4817884e-03, -1.3597310e-03, -1.0780990e-03, -3.3727288e-04,\n",
       "       -1.2394786e-03, -1.3014078e-03, -1.1755228e-03, -1.4871061e-03,\n",
       "       -5.4961443e-04, -1.0651946e-03, -7.2869658e-04, -2.1170974e-03,\n",
       "       -1.5206039e-03, -6.3607097e-04, -2.9477477e-04, -9.7396970e-04,\n",
       "       -1.4373660e-03, -7.1850419e-04, -1.2194514e-03, -3.1650066e-05,\n",
       "       -1.2290776e-03, -1.6688406e-03, -2.0781755e-03, -7.5212121e-04,\n",
       "       -7.5927377e-04, -8.1089139e-04, -5.7670474e-04, -7.8323483e-04,\n",
       "       -6.7359209e-04, -9.4524026e-04, -8.4528327e-04, -2.3219585e-03,\n",
       "       -1.6062558e-03, -9.8836422e-04, -6.2751770e-04, -7.1108341e-04,\n",
       "       -1.5888512e-03, -1.0793209e-03, -1.2972653e-03, -4.4479966e-04,\n",
       "       -1.6540289e-03, -1.1005998e-04, -7.4657798e-04, -1.1738539e-03,\n",
       "       -1.3340116e-03, -2.0594597e-03, -1.0328293e-03, -9.3141198e-04,\n",
       "       -1.3835430e-03, -1.8362403e-03, -9.5438957e-04, -4.6092272e-04,\n",
       "       -2.5472045e-04, -1.3056099e-03, -8.5443258e-04, -8.9707971e-04,\n",
       "       -1.0876656e-03, -6.6879392e-04, -4.3699145e-04, -3.5476685e-04,\n",
       "       -2.0037293e-03, -1.1243522e-03, -2.6432574e-03, -1.9035339e-03,\n",
       "       -8.0034137e-04, -5.5396557e-04, -8.8834763e-04, -9.0599060e-04,\n",
       "       -3.8143992e-04, -1.5857220e-03, -1.5340149e-03, -8.8202953e-04,\n",
       "       -6.1178207e-04, -2.4184287e-03,  4.3034554e-05, -7.5006485e-04,\n",
       "       -6.2838197e-04, -9.0321898e-04, -1.5181005e-03, -1.6836822e-03,\n",
       "       -7.0613623e-04, -6.7594647e-04, -1.5918911e-03, -2.4126768e-03,\n",
       "        1.5363097e-04, -1.0012090e-03, -1.7653704e-03, -2.6762486e-04,\n",
       "       -1.1254549e-03, -2.0826161e-03, -1.2534261e-03, -9.6091628e-04,\n",
       "       -4.4631958e-04, -1.8189251e-03, -2.1135211e-03, -1.0758638e-03,\n",
       "       -1.4823079e-03, -8.3911419e-04, -1.4466941e-03, -9.2336535e-04,\n",
       "       -1.0710061e-03, -1.0764003e-03,  4.3931603e-04, -1.5374422e-03,\n",
       "       -1.4736354e-03, -7.8234076e-04, -8.0442429e-04, -1.8585920e-03,\n",
       "       -2.1720529e-03, -7.4109435e-04, -3.3113360e-04, -1.3279319e-03,\n",
       "       -9.3194842e-04, -8.9502335e-04, -1.5015304e-03, -1.4758408e-03,\n",
       "       -6.6566467e-04, -7.3543191e-04, -1.9491911e-03, -2.2521615e-04,\n",
       "       -1.2095273e-03, -1.0317862e-03, -6.1005354e-04, -1.2596548e-03,\n",
       "       -8.4426999e-04, -7.2765350e-04, -1.2731254e-03, -8.8188052e-04,\n",
       "       -1.8388629e-03, -9.7882748e-04, -1.4664233e-03, -1.8454492e-03,\n",
       "       -9.7927451e-04, -1.2483895e-03, -1.8292069e-03, -1.3165772e-03,\n",
       "       -9.8055601e-04, -2.9671192e-04, -2.2551417e-04, -8.8167191e-04,\n",
       "       -5.4210424e-04, -1.2193322e-03, -2.0277500e-04, -1.2226999e-03,\n",
       "       -9.9626184e-04, -4.3964386e-04, -1.4039576e-03, -1.2643933e-03,\n",
       "       -2.0794570e-03, -6.3002110e-04, -1.4649630e-03, -1.4496148e-03,\n",
       "       -1.4151037e-03, -1.9556284e-03, -3.1024218e-04, -8.2507730e-04,\n",
       "       -4.0999055e-04, -7.3957443e-04, -6.4396858e-04, -1.5338957e-03,\n",
       "       -7.1555376e-04, -1.6353130e-03, -1.8851757e-03, -5.7789683e-04,\n",
       "       -1.5350282e-03, -4.7501922e-04, -6.3633919e-04, -1.6680360e-03,\n",
       "       -2.0590425e-04, -1.3223588e-03, -1.0539293e-03, -7.0065260e-04,\n",
       "       -6.4516068e-04, -1.3037026e-03, -1.6557574e-03, -8.8638067e-04,\n",
       "       -6.3645840e-04, -1.7434955e-03, -2.0712912e-03, -1.1623204e-03,\n",
       "       -1.2420714e-03, -1.2370944e-03, -2.3862720e-04, -1.5912950e-03,\n",
       "       -5.7131052e-04, -7.6237321e-04, -7.6353550e-04,  2.6166439e-05,\n",
       "       -3.4952164e-04, -1.3281703e-03, -1.4975369e-03, -1.7515123e-03,\n",
       "       -1.4335215e-03, -3.1071901e-04, -1.1063516e-03, -1.3884902e-03,\n",
       "       -9.4124675e-04, -9.1728568e-04, -1.3112128e-03, -1.8890798e-03,\n",
       "       -1.0047257e-03, -4.8062205e-04, -8.0466270e-06, -1.8471479e-03,\n",
       "       -6.5922737e-04, -1.8528402e-03, -1.4067292e-03, -1.5324056e-03,\n",
       "       -6.2716007e-04, -5.7381392e-04, -7.4219704e-04, -5.2732229e-04,\n",
       "       -1.1503696e-03, -6.4069033e-04, -1.4563501e-03, -8.1822276e-04,\n",
       "       -1.4986694e-03, -1.2186170e-03, -1.0210872e-03, -6.2954426e-04,\n",
       "       -1.2837648e-03, -9.0363622e-04, -1.1725128e-03, -1.6359389e-03,\n",
       "       -1.5317202e-03, -2.0560920e-03, -4.1246414e-05, -1.0082424e-03,\n",
       "       -8.5085630e-04, -7.2917342e-04, -1.1710823e-03, -6.8777800e-04,\n",
       "       -5.2508712e-04, -6.0024858e-04, -1.0150969e-03, -1.6840398e-03,\n",
       "       -1.0398924e-03, -1.7971992e-03, -1.1665225e-03, -1.4572442e-03,\n",
       "       -8.5482001e-04, -5.3068995e-04, -9.9727511e-04, -1.0164380e-03,\n",
       "       -6.4724684e-04, -6.1142445e-04], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_one.cate_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bef15b6-13a9-45e5-90bc-dd0d30cb4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94658b2-3950-4ebb-be2a-1de4511265af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = simu.tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ed6648-bd34-4572-98ff-974c8572ab1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70939080-94c7-415e-ae4b-be015f931f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
